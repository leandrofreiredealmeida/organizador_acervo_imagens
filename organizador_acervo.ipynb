{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import piexif\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de metadados das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_metadados_imagens(caminho_pasta):\n",
    "    if not os.path.isdir(caminho_pasta):\n",
    "        print(\"O caminho especificado não é uma pasta válida.\")\n",
    "        return\n",
    "    \n",
    "    arquivos = os.listdir(caminho_pasta)\n",
    "    \n",
    "    metadados = []\n",
    "    \n",
    "    for arquivo in arquivos:\n",
    "        if arquivo.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            caminho_arquivo = os.path.join(caminho_pasta, arquivo)\n",
    "            \n",
    "            try:\n",
    "                with Image.open(caminho_arquivo) as img:\n",
    "                    # Obter a data/hora da imagem\n",
    "                    exif_dict = piexif.load(img.info[\"exif\"])\n",
    "                    data_hora = exif_dict['Exif'][piexif.ExifIFD.DateTimeOriginal].decode('utf-8')\n",
    "                    \n",
    "                    if data_hora:\n",
    "                        data_hora_formatada = datetime.strptime(data_hora, \"%Y:%m:%d %H:%M:%S\")\n",
    "                    else:\n",
    "                        data_hora_formatada = \"N/A\"\n",
    "                    \n",
    "                    # Obter a localização (se disponível)\n",
    "                    if piexif.GPSIFD.GPSLatitude in exif_dict['GPS'] and piexif.GPSIFD.GPSLongitude in exif_dict['GPS']:\n",
    "                        # Converter a localização para graus decimais\n",
    "                        lat = exif_dict['GPS'][piexif.GPSIFD.GPSLatitude]\n",
    "                        lon = exif_dict['GPS'][piexif.GPSIFD.GPSLongitude]\n",
    "                        lat_deg = lat[0][0] / lat[0][1] + lat[1][0] / lat[1][1] / 60 + lat[2][0] / lat[2][1] / 3600\n",
    "                        lon_deg = lon[0][0] / lon[0][1] + lon[1][0] / lon[1][1] / 60 + lon[2][0] / lon[2][1] / 3600\n",
    "                        \n",
    "                        # Se a latitude está no hemisfério sul ou a longitude está no hemisfério oeste, tornar o valor negativo\n",
    "                        if exif_dict['GPS'][piexif.GPSIFD.GPSLatitudeRef].decode('utf-8') == 'S': lat_deg = -lat_deg\n",
    "                        if exif_dict['GPS'][piexif.GPSIFD.GPSLongitudeRef].decode('utf-8') == 'W': lon_deg = -lon_deg\n",
    "                        \n",
    "                        localizacao_formatada = f\"{lat_deg}°, {lon_deg}°\"\n",
    "                    else:\n",
    "                        localizacao_formatada = \"N/A\"\n",
    "                    \n",
    "                    metadados.append([arquivo, data_hora_formatada, localizacao_formatada])\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {arquivo}: {e}\")\n",
    "                metadados.append([arquivo, \"N/A\", \"N/A\"])\n",
    "    \n",
    "    with open(os.path.join(caminho_pasta, \"info_imagens.csv\"), mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"nome_arquivo\", \"data_hora\", \"localizacao\"])\n",
    "        writer.writerows(metadados)\n",
    "    \n",
    "    print(\"Metadados extraídos e salvos em info_imagens.csv com sucesso.\")\n",
    "\n",
    "# Adicionar o caminho da pasta\n",
    "caminho_pasta = \"/adicionar/caminho/pasta\"\n",
    "extrair_metadados_imagens(caminho_pasta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficos das informações dos metadados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_info_imagens(caminho_csv):\n",
    "    try:\n",
    "        df = pd.read_csv(caminho_csv)\n",
    "    except FileNotFoundError:\n",
    "        print(\"O arquivo CSV não foi encontrado.\")\n",
    "        return\n",
    "    \n",
    "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
    "    \n",
    "    # Gráfico de barras para distribuição das datas/horas\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['data_hora'].dt.date.value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Distribuição das datas/horas das imagens')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Quantidade de imagens')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Gráfico de dispersão para localização das imagens (se disponível)\n",
    "    df = df.dropna(subset=['localizacao'])\n",
    "    if not df.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        localizacao = df['localizacao'].str.replace('°', '').str.split(', ').apply(pd.Series)\n",
    "        plt.scatter(localizacao[1].astype(float), localizacao[0].astype(float), alpha=0.5)\n",
    "        plt.title('Localização das imagens')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Nenhuma informação de localização disponível.\")\n",
    "\n",
    "caminho_csv = \"/adicionar/caminho/pasta/info_imagens.csv\"\n",
    "visualizar_info_imagens(caminho_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessamento_imagens(caminho_pasta_origem, tamanho_novo=(200, 200)):\n",
    "    if not os.path.isdir(caminho_pasta_origem):\n",
    "        print(\"O caminho especificado não é uma pasta válida.\")\n",
    "        return\n",
    "    \n",
    "    # Criar a pasta para as imagens tratadas\n",
    "    caminho_pasta_tratadas = os.path.join(caminho_pasta_origem, \"imagens_tratadas\")\n",
    "    os.makedirs(caminho_pasta_tratadas, exist_ok=True)\n",
    "    \n",
    "    arquivos = os.listdir(caminho_pasta_origem)\n",
    "    \n",
    "    for arquivo in arquivos:\n",
    "        if arquivo.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            caminho_arquivo_origem = os.path.join(caminho_pasta_origem, arquivo)\n",
    "            \n",
    "            with Image.open(caminho_arquivo_origem) as img:\n",
    "                # Redimensionar a imagem\n",
    "                img_redimensionada = img.resize(tamanho_novo)\n",
    "                \n",
    "                # Converter para escala de cinza\n",
    "                img_cinza = img_redimensionada.convert('L')\n",
    "                \n",
    "                # Normalizar os valores dos pixels (0-255) para (0-1)\n",
    "                img_array = np.array(img_cinza) / 255.0\n",
    "                \n",
    "                # Salvar a imagem tratada na pasta de imagens tratadas\n",
    "                caminho_arquivo_tratado = os.path.join(caminho_pasta_tratadas, arquivo)\n",
    "                img_tratada = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "                img_tratada.save(caminho_arquivo_tratado)\n",
    "    \n",
    "    print(\"Pré-processamento concluído. As imagens tratadas foram salvas em 'imagens_tratadas'.\")\n",
    "\n",
    "caminho_pasta_origem = \"/adicionar/caminho/pasta\"\n",
    "preprocessamento_imagens(caminho_pasta_origem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização - KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio = \"/adicionar/caminho/pasta/imagens_tratadas\"\n",
    "\n",
    "imagens = []\n",
    "\n",
    "for filename in os.listdir(diretorio):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "        img = Image.open(os.path.join(diretorio, filename))\n",
    "        img = np.array(img)\n",
    "        imagens.append(img)\n",
    "\n",
    "imagens = np.array(imagens)\n",
    "\n",
    "imagens_reshaped = imagens.reshape(len(imagens), -1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "kmeans.fit(imagens_reshaped)\n",
    "\n",
    "labels_kmeans = kmeans.labels_\n",
    "\n",
    "print(\"Rótulos dos clusters para cada imagem:\", labels_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização - DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os dados\n",
    "scaler = StandardScaler()\n",
    "imagens_reshaped_scaled = scaler.fit_transform(imagens_reshaped)\n",
    "\n",
    "dbscan = DBSCAN(eps=200, min_samples=5)\n",
    "\n",
    "dbscan.fit(imagens_reshaped_scaled)\n",
    "\n",
    "labels_dbscan = dbscan.labels_\n",
    "\n",
    "print(\"Rótulos dos clusters para cada imagem:\", labels_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação - Coeficiente de Silhueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_kmeans = silhouette_score(imagens_reshaped, labels_kmeans)\n",
    "print(\"Coeficiente médio de silhueta do KMeans: \", silhouette_kmeans)\n",
    "\n",
    "silhouette_dbscan = silhouette_score(imagens_reshaped, labels_dbscan)\n",
    "print(\"Coeficiente médio de silhueta do DBSCAN: \", silhouette_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação - Índice de Davies-Bouldin e Índice de Calinski-Harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o Índice de Davies-Bouldin para KMeans\n",
    "dbi_kmeans = davies_bouldin_score(imagens_reshaped, labels_kmeans)\n",
    "print(\"Índice de Davies-Bouldin para KMeans: \", dbi_kmeans)\n",
    "\n",
    "# Calcular o Índice de Davies-Bouldin para DBSCAN\n",
    "dbi_dbscan = davies_bouldin_score(imagens_reshaped, labels_dbscan)\n",
    "print(\"Índice de Davies-Bouldin para DBSCAN: \", dbi_dbscan)\n",
    "\n",
    "# Calcular o Índice de Calinski-Harabasz para KMeans\n",
    "chi_kmeans = calinski_harabasz_score(imagens_reshaped, labels_kmeans)\n",
    "print(\"Índice de Calinski-Harabasz para KMeans: \", chi_kmeans)\n",
    "\n",
    "# Calcular o Índice de Calinski-Harabasz para DBSCAN\n",
    "chi_dbscan = calinski_harabasz_score(imagens_reshaped, labels_dbscan)\n",
    "print(\"Índice de Calinski-Harabasz para DBSCAN: \", chi_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualização gráfica - Índice de Davies-Bouldin e Índice de Calinski-Harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Algoritmo': ['KMeans', 'KMeans', 'KMeans', 'DBSCAN', 'DBSCAN', 'DBSCAN'],\n",
    "    'Métrica': ['Coeficiente de Silhueta', 'Índice de Davies-Bouldin', 'Índice de Calinski-Harabasz']*2,\n",
    "    'Valor': [silhouette_kmeans, dbi_kmeans, chi_kmeans, silhouette_dbscan, dbi_dbscan, chi_dbscan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['Valor'] = df['Valor'].round(2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x='Métrica', y='Valor', hue='Algoritmo', data=df)\n",
    "plt.title('Comparação das Métricas de Clusterização')\n",
    "\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.2f'), \n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha = 'center', va = 'center', \n",
    "                     xytext = (0, 10), \n",
    "                     textcoords = 'offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organização do acervo conforme o cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/adicionar/caminho/pasta/info_imagens.csv')\n",
    "\n",
    "# Criando as pastas para cada cluster\n",
    "for i in range(3):\n",
    "    os.makedirs(f'/adicionar/caminho/pasta/cluster_{i}', exist_ok=True)\n",
    "\n",
    "# Iterando sobre o DataFrame e copiando as imagens para as pastas correspondentes\n",
    "for i, row in df.iterrows():\n",
    "    if i < len(labels_kmeans):\n",
    "        cluster = labels_kmeans[i]\n",
    "        shutil.copy(f\"/adicionar/caminho/pasta/{row['nome_arquivo']}\", f\"/adicionar/caminho/pasta/cluster_{cluster}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
